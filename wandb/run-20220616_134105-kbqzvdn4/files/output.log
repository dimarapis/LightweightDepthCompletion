task  :  decnet-completion
network_model  :  enet2021
mode  :  eval
dataset  :  nn
workers  :  4
epochs  :  20
criterion  :  l2
batch_size  :  1
learning_rate  :  0.001
weight_decay  :  1e-06
print_freq  :  10
resume  :
data_folder  :  /datasets
convolutional_layer_encoding  :  xyz
dilation_rate  :  2
val_h  :  352
val_w  :  608
min_depth_eval  :  0.1
max_depth_eval  :  80.0
kitti_crop  :  eigen_crop
train_datalist  :  train_dim_kitti.list
val_datalist  :  val_dim_kitti.list
root_folder  :  data/kitti_dataset/val_selection_cropped/
wandblogger  :  False
project  :  depth
entity  :  wandbdimar
STEP 2. Loading datasets...
Loaded 600 training files
Loaded 400 val files
STEP 3. Loading model and metrics...
  8%|█████▎                                                         | 34/400 [00:01<00:17, 20.39it/s]
Loaded model enet2021 for decnet-completion
STEP 4. Training or eval stage...
STEP. Testing block...
???
(0.0, 255.0, 106.76228332519531, 73.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 110.46776580810547, 76.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 108.71294403076172, 87.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 110.58247375488281, 86.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 101.86249542236328, 76.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 88.12545013427734, 57.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 114.40040588378906, 57.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 103.50497436523438, 100.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 108.29024505615234, 120.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 97.3670654296875, 62.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 90.62532043457031, 66.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 109.5428237915039, 80.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 114.7867660522461, 93.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 112.1563949584961, 84.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 113.68180084228516, 118.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 106.5108642578125, 74.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 105.9277572631836, 105.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 98.29277801513672, 70.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 101.16545867919922, 67.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 118.82548522949219, 95.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 99.4951171875, 59.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 84.20145416259766, 57.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 105.58502960205078, 72.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 100.80252838134766, 57.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 96.12840270996094, 63.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 108.12589263916016, 75.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 120.14595794677734, 112.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 102.63533020019531, 80.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 107.92436218261719, 119.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 116.5383529663086, 84.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 105.86384582519531, 78.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 101.495849609375, 71.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 100.14025115966797, 92.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 121.28204345703125, 95.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 102.06805419921875, 74.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 101.0548324584961, 73.0)
 14%|████████▊                                                      | 56/400 [00:02<00:18, 19.07it/s]
Traceback (most recent call last):
  File "main.py", line 316, in <module>
    evaluation_block()
  File "main.py", line 151, in evaluation_block
    for i, data in enumerate(tqdm(test_dl)):
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/tqdm/std.py", line 1180, in __iter__
    for obj in iterable:
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 561, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/dim/depth_2022/features/decnet_dataloaders.py", line 70, in __getitem__
    rgb = np.array(Image.open(self.files[index]['rgb']))
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/PIL/Image.py", line 698, in __array__
    new["data"] = self.tobytes()
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/PIL/Image.py", line 744, in tobytes
    self.load()
  File "/home/dim/anaconda3/envs/pytorchenv/lib/python3.7/site-packages/PIL/ImageFile.py", line 255, in load
    n, err_code = decoder.decode(b)
KeyboardInterrupt
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 103.57223510742188, 73.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 96.35881042480469, 42.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 106.84257507324219, 73.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 100.41426849365234, 71.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 113.85260772705078, 91.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 96.24188995361328, 66.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 111.65931701660156, 80.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 76.63080596923828, 64.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 108.87320709228516, 88.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 127.23831176757812, 112.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 133.91903686523438, 108.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 103.81254577636719, 104.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 94.2502212524414, 87.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 98.3824462890625, 100.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 103.18856048583984, 74.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 108.72744750976562, 60.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 116.20513153076172, 85.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 108.16941833496094, 83.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 102.04933166503906, 48.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])
???
(0.0, 255.0, 106.23233032226562, 65.0)
torch.Size([1, 352, 608, 3])
torch.Size([1, 3, 384, 1280])