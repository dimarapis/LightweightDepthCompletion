| network_model                | AuxSparseGuidedDepth                                                                          |
| pretrained                   | True                                                                                          |
| message                      | testing trained basemodel with refinement Scaler -back to normal weight init - initiallr 1-e5 |
| mode                         | train                                                                                         |
| dataset                      | nn                                                                                            |
| workers                      | 4                                                                                             |
| epochs                       | 100                                                                                           |
| criterion                    | l2                                                                                            |
| batch_size                   | 4                                                                                             |
| learning_rate                | 1e-05                                                                                         |
| weight_decay                 | 0                                                                                             |
| print_freq                   | 10                                                                                            |
| resume                       |                                                                                               |
| data_folder                  | /datasets                                                                                     |
| convolutional_layer_encoding | xyz                                                                                           |
| dilation_rate                | 2                                                                                             |
| val_h                        | 352                                                                                           |
| val_w                        | 608                                                                                           |
| train_height                 | 352                                                                                           |
| train_width                  | 608                                                                                           |
| min_depth_eval               | 0.1                                                                                           |
| max_depth_eval               | 80.0                                                                                          |
| kitti_crop                   | eigen_crop                                                                                    |
| train_datalist               | datalist_train_nn.list                                                                        |
| val_datalist                 | datalist_test_nn.list                                                                         |
| root_folder                  | data/nn_dataset/                                                                              |
| torch_mode                   | pytorch                                                                                       |
| saved_weights                | weights/KITTI_Full_GuideDepth.pth                                                             |
| wandblogger                  | True                                                                                          |
| project                      | depthcompletionpaper                                                                          |
| entity                       | wandbdimar                                                                                    |
| wandbrunname                 | test_ref_sanity                                                                               |

New model saved: weights/2022_07_07-09_12_13_AM/AuxSparseGuidedDepth_1.pth 
| d1       | 0.538733 |
| d2       | 0.763914 |
| d3       | 0.849668 |
| abs_rel  | 0.433262 |
| sq_rel   | 0.683308 |
| rmse     | 1.33634  |
| rmse_log | 0.380725 |
| log10    | 0.139993 |
| silog    | 0.324871 |

New model saved: weights/2022_07_07-09_12_13_AM/AuxSparseGuidedDepth_2.pth 
| d1       | 0.538733 |
| d2       | 0.763914 |
| d3       | 0.849668 |
| abs_rel  | 0.433262 |
| sq_rel   | 0.683308 |
| rmse     | 1.33634  |
| rmse_log | 0.380725 |
| log10    | 0.139993 |
| silog    | 0.324871 |

New model saved: weights/2022_07_07-09_12_13_AM/AuxSparseGuidedDepth_3.pth 
| d1       | 0.538733 |
| d2       | 0.763914 |
| d3       | 0.849668 |
| abs_rel  | 0.433262 |
| sq_rel   | 0.683308 |
| rmse     | 1.33634  |
| rmse_log | 0.380725 |
| log10    | 0.139993 |
| silog    | 0.324871 |

New model saved: weights/2022_07_07-09_12_13_AM/AuxSparseGuidedDepth_4.pth 
| d1       | 0.538733 |
| d2       | 0.763914 |
| d3       | 0.849668 |
| abs_rel  | 0.433262 |
| sq_rel   | 0.683308 |
| rmse     | 1.33634  |
| rmse_log | 0.380725 |
| log10    | 0.139993 |
| silog    | 0.324871 |

New model saved: weights/2022_07_07-09_12_13_AM/AuxSparseGuidedDepth_5.pth 
| d1       | 0.538733 |
| d2       | 0.763914 |
| d3       | 0.849668 |
| abs_rel  | 0.433262 |
| sq_rel   | 0.683308 |
| rmse     | 1.33634  |
| rmse_log | 0.380725 |
| log10    | 0.139993 |
| silog    | 0.324871 |

New model saved: weights/2022_07_07-09_12_13_AM/AuxSparseGuidedDepth_6.pth 
| d1       | 0.538733 |
| d2       | 0.763914 |
| d3       | 0.849668 |
| abs_rel  | 0.433262 |
| sq_rel   | 0.683308 |
| rmse     | 1.33634  |
| rmse_log | 0.380725 |
| log10    | 0.139993 |
| silog    | 0.324871 |

New model saved: weights/2022_07_07-09_12_13_AM/AuxSparseGuidedDepth_7.pth 
| d1       | 0.538733 |
| d2       | 0.763914 |
| d3       | 0.849668 |
| abs_rel  | 0.433262 |
| sq_rel   | 0.683308 |
| rmse     | 1.33634  |
| rmse_log | 0.380725 |
| log10    | 0.139993 |
| silog    | 0.324871 |